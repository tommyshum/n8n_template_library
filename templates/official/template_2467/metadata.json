{
  "id": 2467,
  "name": "Narrating over a Video using Multimodal AI",
  "description": "This n8n template takes a video and extracts frames from it which are used with a multimodal LLM to generate a script. The script is then passed to the same multimodal LLM to generate a voiceover clip.\n\nThis template was inspired by Processing and narrating a video with GPT's visual capabilities and the TTS API\n\nHow it works\nVideo is downloaded using the HTTP node.\nPython code node is used to extract the frames using OpenCV.\nLoop node is used o batch the frames for the LLM to generate partial scripts.\nAll partial scripts are combined to form the full script which is then sent to OpenAI to generate audio from it.\nThe finished voiceover clip is uploaded to Google Drive.\n\nSample the finished product here: https://drive.google.com/file/d/1-XCoii0leGB2MffBMPpCZoxboVyeyeIX/view?usp=sharing\n\nRequirements\n\nOpenAI for LLM\nIdeally, a mid-range (16GB RAM) machine for acceptable performance!\n\nCustomising this workflow\n\nFor larger videos, consider splitting into smaller clips for better performance\nUse a multimodal LLM which supports fully video such as Google's Gemini.\n",
  "totalViews": 8055,
  "source": "official",
  "user": {
    "id": 91804,
    "name": "Jimleuk",
    "username": "jimleuk",
    "bio": "Freelance consultant based in the UK specialising in AI-powered automations. I work with select clients tackling their most challenging projects. For business enquiries, send me an email at hello@jimle.uk\n\nLinkedIn: https://www.linkedin.com/in/jimleuk/\nX/Twitter: https://x.com/jimle_uk",
    "verified": true,
    "links": "[\"\"]",
    "avatar": "https://gravatar.com/avatar/4ab99e51473df76838beeaac908747f7928c625f869794815cabe34016967d51?r=pg&d=retro&size=200"
  },
  "categories": [
    "Marketing",
    "Core Nodes",
    "Development",
    "Data & Storage",
    "AI",
    "Langchain"
  ],
  "nodes": [
    {
      "name": "Edit Image",
      "type": "n8n-nodes-base.editImage",
      "categories": [
        "Marketing",
        "Core Nodes"
      ],
      "subcategories": {
        "Core Nodes": [
          "Files",
          "Data Transformation"
        ]
      }
    },
    {
      "name": "HTTP Request",
      "type": "n8n-nodes-base.httpRequest",
      "categories": [
        "Development",
        "Core Nodes"
      ],
      "subcategories": {
        "Core Nodes": [
          "Helpers"
        ]
      }
    },
    {
      "name": "Google Drive",
      "type": "n8n-nodes-base.googleDrive",
      "categories": [
        "Data & Storage"
      ],
      "subcategories": {}
    },
    {
      "name": "Code",
      "type": "n8n-nodes-base.code",
      "categories": [
        "Development",
        "Core Nodes"
      ],
      "subcategories": {
        "Core Nodes": [
          "Helpers",
          "Data Transformation"
        ]
      }
    },
    {
      "name": "Basic LLM Chain",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "categories": [
        "AI",
        "Langchain"
      ],
      "subcategories": {
        "AI": [
          "Chains",
          "Root Nodes"
        ]
      }
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "categories": [
        "AI",
        "Langchain"
      ],
      "subcategories": {
        "AI": [
          "Language Models",
          "Root Nodes"
        ],
        "Language Models": [
          "Chat Models (Recommended)"
        ]
      }
    },
    {
      "name": "OpenAI",
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "categories": [
        "AI",
        "Langchain"
      ],
      "subcategories": {
        "AI": [
          "Agents",
          "Miscellaneous",
          "Root Nodes"
        ]
      }
    }
  ],
  "nodeCount": 7,
  "createdAt": "2024-10-17T10:45:39.624Z",
  "path": "official/template_2467/workflow.json"
}