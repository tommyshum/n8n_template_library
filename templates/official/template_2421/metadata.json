{
  "id": 2421,
  "name": "Transcribing Bank Statements To Markdown Using Gemini Vision AI",
  "description": "This n8n workflow demonstrates an approach to parsing bank statement PDFs with multimodal LLMs as an alternative to traditional OCR. This allows for much more accurate data extraction from the document especially when it comes to tables and complex layouts.\n\nMultimodal Parsing is better than traditiona OCR because:\nIt reduces complexity and overhead by avoiding the need to preprocess the document into text format such as markdown before passing to the LLM.\nIt handles non-standard PDF formats which may produce garbled output via traditional OCR text conversion.\nIt's orders of magnitude cheaper than premium OCR models that still require post-processing cleanup and formatting. LLMs can format to any schema or language you desire!\n\nHow it works\n\nYou can use the example bank statement created specifically for this workflow here: https://drive.google.com/file/d/1wS9U7MQDthj57CvEcqG_Llkr-ek6RqGA/view?usp=sharing\n\nA PDF bank statement is imported via Google Drive. For this demo, I've created a mock bank statement which includes complex table layouts of 5 columns. Typically, OCR will be unable to align the columns correctly and mistake some deposits for withdrawals.\nBecause multimodal LLMs do not accept PDFs directly, well have to convert the PDF to a series of images. We can achieve this by using a tool such as Stirling PDF. Stirling PDF is self-hostable which is handy for sensitive data such as bank statements.\nStirling PDF will return our PDF as a series of JPGs (one for each page) in a zipped file. We can use n8n's decompress node to extract the images and ensure they are ordered by using the Sort node.\nNext, we'll resize each page using the Edit Image node to ensure the right balance between resolution limits and processing speed.\nEach resized page image is then passed into the Basic LLM node which will use our multimodal LLM of choice - Gemini 1.5 Pro. In the LLM node's options, we'll add a \"user message\" of type binary (data) which is how we add our image data as an input.\nOur prompt will instruct the multimodal LLM to transcribe each page to markdown. Note, you do not need to do this - you can just ask for data points to extract directly! Our goal for this template is to demonstrate the LLMs ability to accurately read the page.\nFinally, with our markdown version of all pages, we can pass this to another LLM node to extract required data such as deposit line items.\n\nRequirements\n\nGoogle Gemini API for Multimodal LLM.\nGoogle Drive access for document storage.\nStirling PDF instance for PDF to Image conversion\n\nCustomising the workflow\n\nAt time of writing, Gemini 1.5 Pro is the most accurate in text document parsing with a relatively low cost. If you are not using Google Gemini however you can switch to other multimodal LLMs such as OpenAI GPT or Antrophic Claude. \n\nIf you don't need the markdown, simply asking what to extract directly in the LLM's prompt is also acceptable and would save a few extra steps.\n\nNot parsing any bank statements any time soon? This template also works for Invoices, inventory lists, contracts, legal documents etc.",
  "totalViews": 12456,
  "source": "official",
  "user": {
    "id": 91804,
    "name": "Jimleuk",
    "username": "jimleuk",
    "bio": "Freelance consultant based in the UK specialising in AI-powered automations. I work with select clients tackling their most challenging projects. For business enquiries, send me an email at hello@jimle.uk\n\nLinkedIn: https://www.linkedin.com/in/jimleuk/\nX/Twitter: https://x.com/jimle_uk",
    "verified": true,
    "links": "[\"\"]",
    "avatar": "https://gravatar.com/avatar/4ab99e51473df76838beeaac908747f7928c625f869794815cabe34016967d51?r=pg&d=retro&size=200"
  },
  "categories": [
    "Marketing",
    "Core Nodes",
    "Development",
    "Data & Storage",
    "AI",
    "Langchain"
  ],
  "nodes": [
    {
      "name": "Edit Image",
      "type": "n8n-nodes-base.editImage",
      "categories": [
        "Marketing",
        "Core Nodes"
      ],
      "subcategories": {
        "Core Nodes": [
          "Files",
          "Data Transformation"
        ]
      }
    },
    {
      "name": "HTTP Request",
      "type": "n8n-nodes-base.httpRequest",
      "categories": [
        "Development",
        "Core Nodes"
      ],
      "subcategories": {
        "Core Nodes": [
          "Helpers"
        ]
      }
    },
    {
      "name": "Google Drive",
      "type": "n8n-nodes-base.googleDrive",
      "categories": [
        "Data & Storage"
      ],
      "subcategories": {}
    },
    {
      "name": "Compression",
      "type": "n8n-nodes-base.compression",
      "categories": [
        "Core Nodes",
        "Data & Storage"
      ],
      "subcategories": {
        "Core Nodes": [
          "Files",
          "Data Transformation"
        ]
      }
    },
    {
      "name": "Code",
      "type": "n8n-nodes-base.code",
      "categories": [
        "Development",
        "Core Nodes"
      ],
      "subcategories": {
        "Core Nodes": [
          "Helpers",
          "Data Transformation"
        ]
      }
    },
    {
      "name": "Basic LLM Chain",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "categories": [
        "AI",
        "Langchain"
      ],
      "subcategories": {
        "AI": [
          "Chains",
          "Root Nodes"
        ]
      }
    },
    {
      "name": "Google Gemini Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "categories": [
        "AI",
        "Langchain"
      ],
      "subcategories": {
        "AI": [
          "Language Models",
          "Root Nodes"
        ],
        "Language Models": [
          "Chat Models (Recommended)"
        ]
      }
    },
    {
      "name": "Information Extractor",
      "type": "@n8n/n8n-nodes-langchain.informationExtractor",
      "categories": [
        "AI",
        "Langchain"
      ],
      "subcategories": {
        "AI": [
          "Chains",
          "Root Nodes"
        ]
      }
    }
  ],
  "nodeCount": 8,
  "createdAt": "2024-09-18T15:34:00.441Z",
  "path": "official/template_2421/workflow.json"
}